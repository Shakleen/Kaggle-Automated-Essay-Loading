{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data_tools.word_feature_engineering import all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34437"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634974"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subwords = []\n",
    "\n",
    "for word in all_words:\n",
    "    for length in range(2, len(word)):\n",
    "        for start in range(len(word) - length):\n",
    "            subwords.append(word[start:start+length].lower())\n",
    "\n",
    "len(subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_counts = Counter(subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_subwords = set(subword for subword, _ in subword_counts.most_common(2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword_counts.get(\"tion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1108"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_score(word):\n",
    "    def get_subword_score(word):\n",
    "        if not word or len(word) < 2:\n",
    "            return 0\n",
    "\n",
    "        max_score = 0\n",
    "\n",
    "        for length in range(2, len(word)):\n",
    "            for start in range(len(word) - length):\n",
    "                subword = word[start : start + length]\n",
    "                score = subword_counts.get(subword, 0)\n",
    "                score += get_subword_score(word[start + length :])\n",
    "                max_score = max(max_score, score)\n",
    "\n",
    "        return max_score\n",
    "    \n",
    "    return get_subword_score(word.lower())\n",
    "\n",
    "get_score(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple 1108\n",
      "Levenshtein 7431\n",
      "notebook 3668\n",
      "school 1271\n",
      "Shakleen 2318\n",
      "Ishfar 2114\n",
      "working 7616\n",
      "studying 8034\n",
      "Living 7705\n",
      "Abacaxi 1808\n",
      "sexy 1266\n",
      "interestingly 19606\n"
     ]
    }
   ],
   "source": [
    "word_list = [\n",
    "    \"apple\",\n",
    "    \"Levenshtein\",\n",
    "    \"notebook\",\n",
    "    \"school\",\n",
    "    \"Shakleen\",\n",
    "    \"Ishfar\",\n",
    "    \"working\",\n",
    "    \"studying\",\n",
    "    \"Living\",\n",
    "    \"Abacaxi\",\n",
    "    \"sexy\",\n",
    "    \"interestingly\"\n",
    "]\n",
    "\n",
    "for word in word_list:\n",
    "    print(word, get_score(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "## constants\n",
    "SLAX = {\n",
    "    \"IH1\",\n",
    "    \"IH2\",\n",
    "    \"EH1\",\n",
    "    \"EH2\",\n",
    "    \"AE1\",\n",
    "    \"AE2\",\n",
    "    \"AH1\",\n",
    "    \"AH2\",\n",
    "    \"UH1\",\n",
    "    \"UH2\",\n",
    "}\n",
    "VOWELS = {\n",
    "    \"IY1\",\n",
    "    \"IY2\",\n",
    "    \"IY0\",\n",
    "    \"EY1\",\n",
    "    \"EY2\",\n",
    "    \"EY0\",\n",
    "    \"AA1\",\n",
    "    \"AA2\",\n",
    "    \"AA0\",\n",
    "    \"ER1\",\n",
    "    \"ER2\",\n",
    "    \"ER0\",\n",
    "    \"AW1\",\n",
    "    \"AW2\",\n",
    "    \"AW0\",\n",
    "    \"AO1\",\n",
    "    \"AO2\",\n",
    "    \"AO0\",\n",
    "    \"AY1\",\n",
    "    \"AY2\",\n",
    "    \"AY0\",\n",
    "    \"OW1\",\n",
    "    \"OW2\",\n",
    "    \"OW0\",\n",
    "    \"OY1\",\n",
    "    \"OY2\",\n",
    "    \"OY0\",\n",
    "    \"IH0\",\n",
    "    \"EH0\",\n",
    "    \"AE0\",\n",
    "    \"AH0\",\n",
    "    \"UH0\",\n",
    "    \"UW1\",\n",
    "    \"UW2\",\n",
    "    \"UW0\",\n",
    "    \"UW\",\n",
    "    \"IY\",\n",
    "    \"EY\",\n",
    "    \"AA\",\n",
    "    \"ER\",\n",
    "    \"AW\",\n",
    "    \"AO\",\n",
    "    \"AY\",\n",
    "    \"OW\",\n",
    "    \"OY\",\n",
    "    \"UH\",\n",
    "    \"IH\",\n",
    "    \"EH\",\n",
    "    \"AE\",\n",
    "    \"AH\",\n",
    "    \"UH\",\n",
    "} | SLAX\n",
    "\n",
    "## licit medial onsets\n",
    "\n",
    "O2 = {\n",
    "    (\"P\", \"R\"),\n",
    "    (\"T\", \"R\"),\n",
    "    (\"K\", \"R\"),\n",
    "    (\"B\", \"R\"),\n",
    "    (\"D\", \"R\"),\n",
    "    (\"G\", \"R\"),\n",
    "    (\"F\", \"R\"),\n",
    "    (\"TH\", \"R\"),\n",
    "    (\"P\", \"L\"),\n",
    "    (\"K\", \"L\"),\n",
    "    (\"B\", \"L\"),\n",
    "    (\"G\", \"L\"),\n",
    "    (\"F\", \"L\"),\n",
    "    (\"S\", \"L\"),\n",
    "    (\"K\", \"W\"),\n",
    "    (\"G\", \"W\"),\n",
    "    (\"S\", \"W\"),\n",
    "    (\"S\", \"P\"),\n",
    "    (\"S\", \"T\"),\n",
    "    (\"S\", \"K\"),\n",
    "    (\"HH\", \"Y\"),  # \"clerihew\"\n",
    "    (\"R\", \"W\"),\n",
    "}\n",
    "O3 = {(\"S\", \"T\", \"R\"), (\"S\", \"K\", \"L\"), (\"T\", \"R\", \"W\")}  # \"octroi\"\n",
    "\n",
    "# This does not represent anything like a complete list of onsets, but\n",
    "# merely those that need to be maximized in medial position.\n",
    "\n",
    "\n",
    "def syllabify(pron, alaska_rule=True):\n",
    "    \"\"\"\n",
    "    Syllabifies a CMU dictionary (ARPABET) word string\n",
    "\n",
    "    # Alaska rule:\n",
    "    >>> pprint(syllabify('AH0 L AE1 S K AH0'.split())) # Alaska\n",
    "    '-AH0-.L-AE1-S.K-AH0-'\n",
    "    >>> pprint(syllabify('AH0 L AE1 S K AH0'.split(), 0)) # Alaska\n",
    "    '-AH0-.L-AE1-.S K-AH0-'\n",
    "\n",
    "    # huge medial onsets:\n",
    "    >>> pprint(syllabify('M IH1 N S T R AH0 L'.split())) # minstrel\n",
    "    'M-IH1-N.S T R-AH0-L'\n",
    "    >>> pprint(syllabify('AA1  K T R W AA0 R'.split())) # octroi\n",
    "    '-AA1-K.T R W-AA0-R'\n",
    "\n",
    "    # destressing\n",
    "    >>> pprint(destress(syllabify('M IH1 L AH0 T EH2 R IY0'.split())))\n",
    "    'M-IH-.L-AH-.T-EH-.R-IY-'\n",
    "\n",
    "    # normal treatment of 'j':\n",
    "    >>> pprint(syllabify('M EH1 N Y UW0'.split())) # menu\n",
    "    'M-EH1-N.Y-UW0-'\n",
    "    >>> pprint(syllabify('S P AE1 N Y AH0 L'.split())) # spaniel\n",
    "    'S P-AE1-N.Y-AH0-L'\n",
    "    >>> pprint(syllabify('K AE1 N Y AH0 N'.split())) # canyon\n",
    "    'K-AE1-N.Y-AH0-N'\n",
    "    >>> pprint(syllabify('M IH0 N Y UW2 EH1 T'.split())) # minuet\n",
    "    'M-IH0-N.Y-UW2-.-EH1-T'\n",
    "    >>> pprint(syllabify('JH UW1 N Y ER0'.split())) # junior\n",
    "    'JH-UW1-N.Y-ER0-'\n",
    "    >>> pprint(syllabify('K L EH R IH HH Y UW'.split())) # clerihew\n",
    "    'K L-EH-.R-IH-.HH Y-UW-'\n",
    "\n",
    "    # nuclear treatment of 'j'\n",
    "    >>> pprint(syllabify('R EH1 S K Y UW0'.split())) # rescue\n",
    "    'R-EH1-S.K-Y UW0-'\n",
    "    >>> pprint(syllabify('T R IH1 B Y UW0 T'.split())) # tribute\n",
    "    'T R-IH1-B.Y-UW0-T'\n",
    "    >>> pprint(syllabify('N EH1 B Y AH0 L AH0'.split())) # nebula\n",
    "    'N-EH1-B.Y-AH0-.L-AH0-'\n",
    "    >>> pprint(syllabify('S P AE1 CH UH0 L AH0'.split())) # spatula\n",
    "    'S P-AE1-.CH-UH0-.L-AH0-'\n",
    "    >>> pprint(syllabify('AH0 K Y UW1 M AH0 N'.split())) # acumen\n",
    "    '-AH0-K.Y-UW1-.M-AH0-N'\n",
    "    >>> pprint(syllabify('S AH1 K Y AH0 L IH0 N T'.split())) # succulent\n",
    "    'S-AH1-K.Y-AH0-.L-IH0-N T'\n",
    "    >>> pprint(syllabify('F AO1 R M Y AH0 L AH0'.split())) # formula\n",
    "    'F-AO1 R-M.Y-AH0-.L-AH0-'\n",
    "    >>> pprint(syllabify('V AE1 L Y UW0'.split())) # value\n",
    "    'V-AE1-L.Y-UW0-'\n",
    "\n",
    "    # everything else\n",
    "    >>> pprint(syllabify('N AO0 S T AE1 L JH IH0 K'.split())) # nostalgic\n",
    "    'N-AO0-.S T-AE1-L.JH-IH0-K'\n",
    "    >>> pprint(syllabify('CH ER1 CH M AH0 N'.split())) # churchmen\n",
    "    'CH-ER1-CH.M-AH0-N'\n",
    "    >>> pprint(syllabify('K AA1 M P AH0 N S EY2 T'.split())) # compensate\n",
    "    'K-AA1-M.P-AH0-N.S-EY2-T'\n",
    "    >>> pprint(syllabify('IH0 N S EH1 N S'.split())) # inCENSE\n",
    "    '-IH0-N.S-EH1-N S'\n",
    "    >>> pprint(syllabify('IH1 N S EH2 N S'.split())) # INcense\n",
    "    '-IH1-N.S-EH2-N S'\n",
    "    >>> pprint(syllabify('AH0 S EH1 N D'.split())) # ascend\n",
    "    '-AH0-.S-EH1-N D'\n",
    "    >>> pprint(syllabify('R OW1 T EY2 T'.split())) # rotate\n",
    "    'R-OW1-.T-EY2-T'\n",
    "    >>> pprint(syllabify('AA1 R T AH0 S T'.split())) # artist\n",
    "    '-AA1 R-.T-AH0-S T'\n",
    "    >>> pprint(syllabify('AE1 K T ER0'.split())) # actor\n",
    "    '-AE1-K.T-ER0-'\n",
    "    >>> pprint(syllabify('P L AE1 S T ER0'.split())) # plaster\n",
    "    'P L-AE1-S.T-ER0-'\n",
    "    >>> pprint(syllabify('B AH1 T ER0'.split())) # butter\n",
    "    'B-AH1-.T-ER0-'\n",
    "    >>> pprint(syllabify('K AE1 M AH0 L'.split())) # camel\n",
    "    'K-AE1-.M-AH0-L'\n",
    "    >>> pprint(syllabify('AH1 P ER0'.split())) # upper\n",
    "    '-AH1-.P-ER0-'\n",
    "    >>> pprint(syllabify('B AH0 L UW1 N'.split())) # balloon\n",
    "    'B-AH0-.L-UW1-N'\n",
    "    >>> pprint(syllabify('P R OW0 K L EY1 M'.split())) # proclaim\n",
    "    'P R-OW0-.K L-EY1-M'\n",
    "    >>> pprint(syllabify('IH0 N S EY1 N'.split())) # insane\n",
    "    '-IH0-N.S-EY1-N'\n",
    "    >>> pprint(syllabify('IH0 K S K L UW1 D'.split())) # exclude\n",
    "    '-IH0-K.S K L-UW1-D'\n",
    "    \"\"\"\n",
    "    ## main pass\n",
    "    mypron = list(pron)\n",
    "    nuclei = []\n",
    "    onsets = []\n",
    "    i = -1\n",
    "    for j, seg in enumerate(mypron):\n",
    "        if seg in VOWELS:\n",
    "            nuclei.append([seg])\n",
    "            onsets.append(mypron[i + 1 : j])  # actually interludes, r.n.\n",
    "            i = j\n",
    "    codas = [mypron[i + 1 :]]\n",
    "    ## resolve disputes and compute coda\n",
    "    for i in range(1, len(onsets)):\n",
    "        coda = []\n",
    "        # boundary cases\n",
    "        if len(onsets[i]) > 1 and onsets[i][0] == \"R\":\n",
    "            nuclei[i - 1].append(onsets[i].pop(0))\n",
    "        if len(onsets[i]) > 2 and onsets[i][-1] == \"Y\":\n",
    "            nuclei[i].insert(0, onsets[i].pop())\n",
    "        if (\n",
    "            len(onsets[i]) > 1\n",
    "            and alaska_rule\n",
    "            and nuclei[i - 1][-1] in SLAX\n",
    "            and onsets[i][0] == \"S\"\n",
    "        ):\n",
    "            coda.append(onsets[i].pop(0))\n",
    "        # onset maximization\n",
    "        depth = 1\n",
    "        if len(onsets[i]) > 1:\n",
    "            if tuple(onsets[i][-2:]) in O2:\n",
    "                depth = 3 if tuple(onsets[i][-3:]) in O3 else 2\n",
    "        for j in range(len(onsets[i]) - depth):\n",
    "            coda.append(onsets[i].pop(0))\n",
    "        # store coda\n",
    "        codas.insert(i - 1, coda)\n",
    "\n",
    "    ## verify that all segments are included in the ouput\n",
    "    output = list(zip(onsets, nuclei, codas))  # in Python3 zip is a generator\n",
    "    flat_output = list(chain.from_iterable(chain.from_iterable(output)))\n",
    "    if flat_output != mypron:\n",
    "        raise ValueError(f\"could not syllabify {mypron}, got {flat_output}\")\n",
    "    return output\n",
    "\n",
    "\n",
    "def pprint(syllab):\n",
    "    \"\"\"\n",
    "    Pretty-print a syllabification\n",
    "    \"\"\"\n",
    "    return \".\".join(\"-\".join(\" \".join(p) for p in syl) for syl in syllab)\n",
    "\n",
    "\n",
    "def destress(syllab):\n",
    "    \"\"\"\n",
    "    Generate a syllabification with nuclear stress information removed\n",
    "    \"\"\"\n",
    "    syls = []\n",
    "    for onset, nucleus, coda in syllab:\n",
    "        nuke = [p[:-1] if p[-1] in {\"0\", \"1\", \"2\"} else p for p in nucleus]\n",
    "        syls.append((onset, nuke, coda))\n",
    "    return syls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not syllabify ['w', 'o', 'r', 'k', 'i', 'n', 'g'], got []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msyllabify\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mworking\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 230\u001b[0m, in \u001b[0;36msyllabify\u001b[0;34m(pron, alaska_rule)\u001b[0m\n\u001b[1;32m    228\u001b[0m flat_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chain\u001b[38;5;241m.\u001b[39mfrom_iterable(chain\u001b[38;5;241m.\u001b[39mfrom_iterable(output)))\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flat_output \u001b[38;5;241m!=\u001b[39m mypron:\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not syllabify \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmypron\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflat_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mValueError\u001b[0m: could not syllabify ['w', 'o', 'r', 'k', 'i', 'n', 'g'], got []"
     ]
    }
   ],
   "source": [
    "syllabify(\"working\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
