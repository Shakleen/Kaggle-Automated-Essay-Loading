{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèãÔ∏è Model Training - Refit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing from packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ishfar/New Volume/Studies/Projects/Kaggle-Automated-Essay-Scoring/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import wandb\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing user defined packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.config import config\n",
    "from lib.paths import Paths\n",
    "from lib.model.epoch_functions import valid_epoch\n",
    "from lib.model.utils import get_score, get_model_optimizer_and_scheduler\n",
    "from lib.utils.utils import seed_everything\n",
    "from lib.utils.average_meter import AverageMeter\n",
    "from lib.data import (\n",
    "    clean_text,\n",
    "    sliding_window,\n",
    "    negative_sample_df,\n",
    "    get_data_loaders,\n",
    "    collate,\n",
    ")\n",
    "from lib.criterion.metrics import log_metrics\n",
    "from lib.model.inference import ensemble_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(apex=True,\n",
      "          batch_scheduler=True,\n",
      "          batch_size_train=32,\n",
      "          batch_size_valid=32,\n",
      "          betas=[0.9, 0.999],\n",
      "          data_version=4,\n",
      "          debug=False,\n",
      "          decoder_lr=2e-05,\n",
      "          encoder_lr=2e-05,\n",
      "          epochs=2,\n",
      "          eps=1e-06,\n",
      "          gradient_accumulation_steps=1,\n",
      "          gradient_checkpointing=True,\n",
      "          max_grad_norm=1000,\n",
      "          max_length=512,\n",
      "          min_lr=1e-06,\n",
      "          model='microsoft/deberta-v3-xsmall',\n",
      "          n_folds=7,\n",
      "          negative_sample=True,\n",
      "          negative_sample_partitions=3,\n",
      "          oversample=False,\n",
      "          num_classes=6,\n",
      "          num_cycles=0.5,\n",
      "          num_warmup_steps=0,\n",
      "          num_workers=6,\n",
      "          print_freq=6,\n",
      "          random_seed=20,\n",
      "          scheduler='cosine',\n",
      "          stride=192,\n",
      "          tokenizer_version=2,\n",
      "          train=True,\n",
      "          train_folds=[0, 1, 2, 3, 4, 5, 6, 7],\n",
      "          weight_decay=0.01)\n"
     ]
    }
   ],
   "source": [
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåé Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition data has a class imbalance problem. Which is why I assign higher weights to classes having lower samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = torch.tensor([1.0, 0.25, 0.25, 0.5, 1.0, 2.0]).to(device)\n",
    "class_weights = torch.tensor([1.0] * 6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the tokenizer: 128003\n",
      "DebertaV2TokenizerFast(name_or_path='output/microsoft/deberta-v3-xsmall/tokenizer_v2', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t128000: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128001: AddedToken(\"\n",
      "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t128002: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Paths.TOKENIZER_PATH)\n",
    "vocabulary = tokenizer.get_vocab()\n",
    "total_tokens = len(vocabulary)\n",
    "print(\"Total number of tokens in the tokenizer:\", total_tokens)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(Paths.TRAIN_CSV_PATH)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"full_text\"] = df[\"full_text\"].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"score\"] = df[\"score\"].map(lambda x: x - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedShuffleSplit(\n",
    "#     n_splits=1,\n",
    "#     test_size=1000,\n",
    "#     random_state=config.random_seed,\n",
    "# )\n",
    "\n",
    "# train_idx, valid_idx = next(skf.split(df[\"full_text\"], df[\"score\"]))\n",
    "# train_idx.shape, valid_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = df.iloc[train_idx]\n",
    "# valid_df = df.iloc[valid_idx]\n",
    "\n",
    "# train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = sliding_window(train_df, tokenizer)\n",
    "# valid_df = sliding_window(valid_df, tokenizer)\n",
    "# train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(Paths.REFIT_DATA_LOADER_PATH):\n",
    "#     os.makedirs(Paths.REFIT_DATA_LOADER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, sampled_df in negative_sample_df(train_df):\n",
    "#     train_loader, valid_loader = get_data_loaders(sampled_df, valid_df, tokenizer)\n",
    "\n",
    "#     train_dataloader_name = f\"train_{i}.pth\"\n",
    "#     train_dataloader_path = os.path.join(Paths.REFIT_DATA_LOADER_PATH, train_dataloader_name)\n",
    "#     torch.save(train_loader, train_dataloader_path)\n",
    "#     print(f\"Saved {train_dataloader_path} with {len(sampled_df)} samples \")\n",
    "\n",
    "#     valid_dataloader_path = os.path.join(Paths.REFIT_DATA_LOADER_PATH, \"valid.pth\")\n",
    "#     torch.save(valid_loader, valid_dataloader_path)\n",
    "#     print(f\"Saved {valid_dataloader_path} with {len(valid_df)} samples \")\n",
    "\n",
    "#     valid_csv_path = os.path.join(Paths.REFIT_DATA_LOADER_PATH, \"valid.csv\")\n",
    "#     valid_df.to_csv(valid_csv_path, index=False)\n",
    "#     print(f\"Saved {valid_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    step,\n",
    "    inputs,\n",
    "    labels,\n",
    "    criterion,\n",
    "    model,\n",
    "    scaler,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    loss_tracker,\n",
    "    score_tracker,\n",
    "    softmax,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = v.to(device)\n",
    "\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    batch_size = labels.size(0)\n",
    "    with torch.cuda.amp.autocast(enabled=config.apex):\n",
    "        y_preds = model(inputs)\n",
    "        loss = criterion(y_preds, labels)\n",
    "        _, predictions = torch.max(softmax(torch.tensor(y_preds.detach().to(\"cpu\").numpy())), dim=1)\n",
    "        score = get_score(labels.detach().to(\"cpu\").numpy(), predictions)\n",
    "\n",
    "    if config.gradient_accumulation_steps > 1:\n",
    "        loss = loss / config.gradient_accumulation_steps\n",
    "\n",
    "    score_tracker.update(score, batch_size)\n",
    "    loss_tracker.update(loss.item(), batch_size)\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if config.batch_scheduler:\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    loss_tracker = AverageMeter()\n",
    "    score_tracker = AverageMeter()\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    with tqdm(\n",
    "        valid_loader, unit=\"valid_batch\", desc=f\"Validating\"\n",
    "    ) as tqdm_valid_loader:\n",
    "        for batch in tqdm_valid_loader:\n",
    "            inputs = collate(batch.pop(\"inputs\"))\n",
    "            labels = batch.pop(\"labels\")\n",
    "\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(device)\n",
    "\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            batch_size = labels.size(0)\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(inputs)\n",
    "                loss = criterion(y_preds, labels)\n",
    "\n",
    "                _, predictions = torch.max(softmax(torch.tensor(y_preds.detach().to(\"cpu\").numpy())), dim=1)\n",
    "                score = get_score(labels.detach().to(\"cpu\").numpy(), predictions)\n",
    "\n",
    "            if config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / config.gradient_accumulation_steps\n",
    "\n",
    "            loss_tracker.update(loss.item(), batch_size)\n",
    "            score_tracker.update(score, batch_size)\n",
    "\n",
    "    return loss_tracker.avg, score_tracker.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, valid_loader, group):\n",
    "    model, optimizer, scheduler = get_model_optimizer_and_scheduler(\n",
    "        train_loader, device\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config.apex)\n",
    "    loss_tracker = AverageMeter()\n",
    "    score_tracker = AverageMeter()\n",
    "    best_score = -1e8\n",
    "    patience = 5\n",
    "    model_save_path = os.path.join(\n",
    "        Paths.MODEL_OUTPUT_PATH,\n",
    "        f\"{config.model.replace('/', '_')}_{group}.pth\",\n",
    "    )\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Training model no {group}\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            inputs, labels = collate(batch.pop(\"inputs\")), batch.pop(\"labels\")\n",
    "\n",
    "            train_step(\n",
    "                step,\n",
    "                inputs,\n",
    "                labels,\n",
    "                criterion,\n",
    "                model,\n",
    "                scaler,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                loss_tracker,\n",
    "                score_tracker,\n",
    "                softmax,\n",
    "            )\n",
    "\n",
    "            if (step + 1) % 25 == 0:\n",
    "                print(\n",
    "                    f\"Step: {epoch * len(train_loader) + step + 1} \"\n",
    "                    + f\"Train loss: {loss_tracker.avg:<8.6f} \"\n",
    "                    + f\"Train Score: {score_tracker.avg:<8.7f}\"\n",
    "                )\n",
    "                loss_tracker.reset()\n",
    "                score_tracker.reset()\n",
    "\n",
    "            if (step + 1) % 100 == 0:\n",
    "                avg_valid_loss, avg_valid_score = valid_epoch(\n",
    "                    valid_loader,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    device,\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    \"\\t\" * 4\n",
    "                    + f\"Step: {epoch * len(train_loader) + step + 1} \"\n",
    "                    + f\"Valid loss: {avg_valid_loss:<8.6f} \"\n",
    "                    + f\"Valid Score: {avg_valid_score:<8.7f}\"\n",
    "                )\n",
    "\n",
    "                if avg_valid_score > best_score:\n",
    "                    best_score = avg_valid_score\n",
    "                    print(\"\\t\" * 4 + f\"Save Best Score: {best_score:.4f} Model\")\n",
    "                    torch.save(model.state_dict(), model_save_path)\n",
    "                    early_stopping_hook = 0\n",
    "                else:\n",
    "                    early_stopping_hook += 1\n",
    "\n",
    "                    if early_stopping_hook > patience:\n",
    "                        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Training model no 0\n",
      "====================================================================================================\n",
      "Step: 25 Train loss: 1.668365 Train Score: 0.3136072\n",
      "Step: 50 Train loss: 1.595789 Train Score: 0.2681167\n",
      "Step: 75 Train loss: 1.483813 Train Score: 0.2517583\n",
      "Step: 100 Train loss: 1.405148 Train Score: 0.5399034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:07<00:00,  6.08valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tStep: 100 Valid loss: 1.631641 Valid Score: 0.4545698\n",
      "\t\t\t\tSave Best Score: 0.4546 Model\n",
      "Step: 125 Train loss: 1.313972 Train Score: 0.5979896\n",
      "Step: 150 Train loss: 1.231612 Train Score: 0.6472341\n",
      "Step: 175 Train loss: 1.210390 Train Score: 0.6522348\n",
      "Step: 200 Train loss: 1.099617 Train Score: 0.7209933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:07<00:00,  5.68valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tStep: 200 Valid loss: 1.549360 Valid Score: 0.5598667\n",
      "\t\t\t\tSave Best Score: 0.5599 Model\n",
      "Step: 225 Train loss: 1.094108 Train Score: 0.7290337\n",
      "Step: 250 Train loss: 1.081026 Train Score: 0.7378409\n",
      "Step: 275 Train loss: 1.033602 Train Score: 0.7540578\n",
      "Step: 300 Train loss: 1.045356 Train Score: 0.7605054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:07<00:00,  5.63valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tStep: 300 Valid loss: 1.127172 Valid Score: 0.7061128\n",
      "\t\t\t\tSave Best Score: 0.7061 Model\n",
      "Step: 347 Train loss: 1.032314 Train Score: 0.7645337\n",
      "Step: 372 Train loss: 0.964545 Train Score: 0.7869788\n",
      "Step: 397 Train loss: 0.926067 Train Score: 0.8211608\n",
      "Step: 422 Train loss: 0.945478 Train Score: 0.8142749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:07<00:00,  5.61valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tStep: 422 Valid loss: 1.098279 Valid Score: 0.7181316\n",
      "\t\t\t\tSave Best Score: 0.7181 Model\n",
      "Step: 447 Train loss: 0.916817 Train Score: 0.8058800\n",
      "Step: 472 Train loss: 0.953790 Train Score: 0.8068172\n",
      "Step: 497 Train loss: 0.905455 Train Score: 0.8105239\n",
      "Step: 522 Train loss: 0.937706 Train Score: 0.8085786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:07<00:00,  5.61valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tStep: 522 Valid loss: 1.026089 Valid Score: 0.7315657\n",
      "\t\t\t\tSave Best Score: 0.7316 Model\n",
      "Step: 547 Train loss: 0.843696 Train Score: 0.8268022\n",
      "Step: 572 Train loss: 0.896103 Train Score: 0.8124611\n",
      "Step: 597 Train loss: 0.889271 Train Score: 0.8109945\n",
      "Step: 622 Train loss: 0.845814 Train Score: 0.8370697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:07<00:00,  5.62valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tStep: 622 Valid loss: 1.114254 Valid Score: 0.7163236\n",
      "Step: 669 Train loss: 0.826415 Train Score: 0.8448669\n",
      "Step: 694 Train loss: 0.823945 Train Score: 0.8549754\n",
      "Step: 719 Train loss: 0.884044 Train Score: 0.8299577\n",
      "Step: 744 Train loss: 0.830496 Train Score: 0.8329360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:07<00:00,  5.80valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tStep: 744 Valid loss: 1.183954 Valid Score: 0.7031672\n",
      "Step: 769 Train loss: 0.782057 Train Score: 0.8570753\n",
      "Step: 794 Train loss: 0.787777 Train Score: 0.8507694\n",
      "Step: 819 Train loss: 0.867009 Train Score: 0.8296344\n",
      "Step: 844 Train loss: 0.777243 Train Score: 0.8503658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:07<00:00,  5.59valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tStep: 844 Valid loss: 1.001806 Valid Score: 0.7606428\n",
      "\t\t\t\tSave Best Score: 0.7606 Model\n",
      "Step: 869 Train loss: 0.766312 Train Score: 0.8559080\n",
      "Step: 894 Train loss: 0.841452 Train Score: 0.8399428\n",
      "Step: 919 Train loss: 0.793294 Train Score: 0.8520326\n",
      "Step: 944 Train loss: 0.688452 Train Score: 0.8847934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:07<00:00,  5.68valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tStep: 944 Valid loss: 0.982088 Valid Score: 0.7657854\n",
      "\t\t\t\tSave Best Score: 0.7658 Model\n",
      "Step: 991 Train loss: 0.727722 Train Score: 0.8712275\n",
      "Step: 1016 Train loss: 0.730045 Train Score: 0.8649657\n",
      "Step: 1041 Train loss: 0.653481 Train Score: 0.8791255\n",
      "Step: 1066 Train loss: 0.632664 Train Score: 0.8766852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:07<00:00,  5.57valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tStep: 1066 Valid loss: 1.024375 Valid Score: 0.7599330\n",
      "Step: 1091 Train loss: 0.709419 Train Score: 0.8696680\n",
      "Step: 1116 Train loss: 0.714261 Train Score: 0.8622958\n",
      "Step: 1141 Train loss: 0.724121 Train Score: 0.8620969\n",
      "Step: 1166 Train loss: 0.718790 Train Score: 0.8508093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:07<00:00,  5.66valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tStep: 1166 Valid loss: 1.322416 Valid Score: 0.6960582\n",
      "Step: 1191 Train loss: 0.756524 Train Score: 0.8399147\n",
      "Step: 1216 Train loss: 0.640135 Train Score: 0.8890805\n",
      "Step: 1241 Train loss: 0.663968 Train Score: 0.8822136\n",
      "Step: 1266 Train loss: 0.664545 Train Score: 0.8817361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:07<00:00,  5.70valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tStep: 1266 Valid loss: 0.992712 Valid Score: 0.7584647\n",
      "Step: 1313 Train loss: 0.635364 Train Score: 0.8815894\n",
      "Step: 1338 Train loss: 0.589857 Train Score: 0.8921267\n",
      "Step: 1363 Train loss: 0.560935 Train Score: 0.8932134\n",
      "Step: 1388 Train loss: 0.605351 Train Score: 0.8770950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:07<00:00,  5.69valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tStep: 1388 Valid loss: 1.101825 Valid Score: 0.7500285\n",
      "Step: 1413 Train loss: 0.598124 Train Score: 0.8804385\n",
      "Step: 1438 Train loss: 0.577070 Train Score: 0.8821380\n",
      "Step: 1463 Train loss: 0.600212 Train Score: 0.8838637\n",
      "Step: 1488 Train loss: 0.616666 Train Score: 0.8765288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:07<00:00,  5.95valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tStep: 1488 Valid loss: 1.104084 Valid Score: 0.7523852\n",
      "Step: 1513 Train loss: 0.589711 Train Score: 0.8981576\n",
      "Step: 1538 Train loss: 0.601114 Train Score: 0.8771168\n"
     ]
    }
   ],
   "source": [
    "valid_loader = torch.load(os.path.join(Paths.REFIT_DATA_LOADER_PATH, \"valid.pth\"))\n",
    "\n",
    "for i in range(config.negative_sample_partitions):\n",
    "    train_loader = torch.load(\n",
    "        os.path.join(Paths.REFIT_DATA_LOADER_PATH, f\"train_{i}.pth\")\n",
    "    )\n",
    "\n",
    "    train_model(train_loader, valid_loader, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
